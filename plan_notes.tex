\documentclass[a4paper,10pt]{article}

\usepackage{custom_packages}
\usepackage{custom_commands}

\hypersetup{ 									
pdfauthor = {}, 
pdftitle = {},
pdfsubject = {}, 
pdfkeywords = {}, 
pdfcreator = {}, 						
pdfproducer = {}, 
plainpages = false}

\newcommand{\note}[1]{\textcolor{blue}{\textbf{#1}}}

\begin{document}
%\title{Methods for Redescription Mining}
%\maketitle
\section{Introduction}
Finding multiple ways to characterize the same entities is a problem that
appears in many areas of science. In medical sciences, for example, one 
typically wants to find a subset of patients sharing similar symptoms and
similar genes. In biology, the bioclimatic constraints that must be met 
for a certain species to survive constitute that species' bioclimatic envelope.

In redescription mining the input contains entities with two sets of
characterizing variables. The task is to find a pair of queries
$(q_A,q_B)$, one query for both sets of variables, such that both
queries describe (almost) the same set of entities. In niche-finding,
for instance, the entities would be spatial locations, one set of
variables would be the fauna and the other set would contain the
bioclimatic variables. A very simple example of a redescription in
this setting could say that the area where polar bears live is the
area where May's mean temperature is between $-3$ and $-7$ degrees
Celsius.

\section{Related Work}

The starting point of many data mining tasks is a dataset that can be
though of as a data matrix where each row represent an object and
each column represent a property of the object.  The terminology used
to refer to the rows and columns varies widely in different contexts.

\paragraph{Objects}
In particular, the objects (Formal Concept Analysis) described by the
data, i.e. the rows, can be called entities (Redescription Mining),
tuples (Exceptional Model Mining), transactions (Frequent Itemset
Mining), individuals (Subgroup Discovery), points (Logical Analysis of
Data) or samples (Multivariate Analysis).  

In addition, given subsets of objects are often called subgroups, in
particular in Subgroup Discovery and Exceptional Model Mining, or
simply group.

\paragraph{Attributes}
The attributes (FCA), i.e. the columns, are called variables (RM),
attributes (FCA, SD, EMM, classification), properties (FCA, SD), items
(FIM) or measurements (MA).

Attributes can be classified dependending on the domain from which
their values are taken.  The domain of binary attributes consists of
only two values and when these values are associated to a Boolean
algebra, the attribute is called Boolean.  Attributes with discrete
domain are called nominal or categorical if the values are unordered
and ordinal when an order can be applied on the values.  In general,
partial orders can be seen as a hierarchies, while total pre-orders
define a ranking of the values.  Real-valued attributes have a
continous domain that can be mapped to a subset of the real numbers.

Attributes related to quantities, that can be measured, are called
quantitative attributes, as opposed to qualitative attributes.

% * Boolean (associated to a Boolean algebra)/ binary (takes two values), 
% * Nominal, categorical (discrete, unordered)
% * Ordinal (discrete, ordered)
% * Real-valued, quantitative (continuous)

In supervised settings, a single attribute or a small set of
attributes of special interest in the task is called target
attribute(s), target class or label.  The term example, is often used
to refer to an object together with its value for the target
attribute. In particular, if the target is binary, it defines two sets
of examples, generally called positive and negative.

\paragraph{Truth Value Assignments}
For an attribute of any of the types defined above it is possible to
construct a truth value assignment by choosing a subset $S$ of values
in the domain of that attribute. Then, if the variable takes a value
in $S$ the assignment is true, otherwise it is false. We denote such a
truth value assignment using Iverson notation $[v \in S]$ or simply
$[v = a]$ when $S$ is a singleton set $\{a\}$. The domain of any
binary variable $v$ can be mapped to $\{\text{True}, \text{False}\}$,
and be naturally interpreted as the truth value assignment $[v \in
\{v_T\}]$ where $v_T$ is the value of $v$ that maps to
$\text{True}$. In the case of a real-valued attribute, the set $S$ can
be an interval, possibly open, for example $S = (a,b] = \{ x \in
\mathbb{R}, a < x \leq b \}$ and the truth value assignement is then
denoted as $[ a < v \leq b]$.

One categorical attribute can be transformed into several binary
variables, one corresponding to each original category, in a process
called binarisation.  Discretization is used to transform a
real-valued attribute into a categorical one; The continuous values
are partitioned to several categorical attributes, each representing
an interval. The intervals can be contiguous, overlapping, cover the
original domain totally or partially, and are sometimes called buckets
or bins and their endpoints are called the lower and upper cut points,
thresholds or bounds.

The new attributes created in this way are clearly dependent and this
fact can be exploited during the analysis of the data. In this
case, the binarisation of categorical attributes does not imply a loss
of information; on the contrary, the discretization of a real-valued
attribute, generally entails a loss of information when several
distinct values are aggregated together.

\paragraph{Query languages}
Such truth value assignments together with their negations are called
features (SD) or literals and can be combined using the Boolean
operators $\land$ and $\lor$ (conjunction and disjunction,
respectively) to form Boolean formulae which are a particular type of
queries.

Queries, also called patterns or descriptors, more generally define a
set of constraints over the values taken by the attributes.  In most
cases, the query language does not consists of all the possible
Boolean formulae but is restricted to a subset of them, the most
typical example being monotone conjunctions, i.e. only positive
literal combined with logical $\land$, other restrictions are Horn
clauses, linear or quadratic formulae.  
On the other hand, requiring
the queries to appear in a normal form, such as DNF or CNF does not
reduce the expressivity of the pattern language.

For a given query, one can determine for each object whether the query
holds true or not, that is whether the object complies with the
constraints it defines.  The set of objects for which the query hold
is called the support of the query. The term support is also sometimes
used to refer to the size of that set.

Extended query languages might be needed for example when dealing with
interval-valued attributes, to reason on inclusion, overlaps, etc. or
when time is taken into consideration, a set of attributes can
correspond to successive measurements of the same characteristic over
time and alternatively to measurements of different characteristics
performed at the same time point.


\note{In most cases the correspondence between the entities and the
attribute values is a one-to-one mapping, that is, to each entity is
associated a single value for each attribute. In multi-relational
data-mining, however, several values might be associated to a single
object for one given attribute.}


\paragraph{Unsupervised tasks}
Given a dataset and a pattern language, the aim of Frequent Pattern
Mining is to enumerate all queries with support larger than a given
threshold.  In particular, Frequent Itemset Mining solves this problem
when the attributes are Boolean and the query language is restricted
to monotone conjunctions, i.e. itemsets.

Formal Concept Analysis considers only closed monotone
conjunctions. In this language, each query is uniquely identified by
the set of attributes over which it is defined, also called the intent
of the Formal Concept. The support of the query is called the extent
of the formal concept.  A Formal Concept correspond to closed monotone
conjunctions together with their support, that is, to a pair extent
and intent.  The spaces of intents and extents can be ordered using
the inclusion relation. Together with the operators that map each
extent to its associated intent and vice-versa, these partially
ordered set form a Galois connection.  The aim of Formal Concept
Analysis is to identify the formal concepts from a dataset and study
the associated connection.

\paragraph{Supervised tasks}

Two supervised learning tasks that can be formulated in this
framework.  They are variants of the classification problem that can
be distinguished by the type of the target attribute.  

First, if the target is a simple binary attribute, the problem of
finding a query that discriminate between objects from the two groups
correspond to the standard binary classification task, also known as
filtering. In other words, a perfect filter, in this setting, would be
a query that holds true for all positive objects but none of the
negative ones.

Second, in the more general case of a nominal target attribute, the
aim is to find a set of query, one for each value of the target
attribute, such that for each object the query corresponding to its
target value holds true and no other.

Of course, the target values are rarely known before hand for all
possible objects and a perfect classifier may not exists. Therefore
the solution of the problem does not lie in finding a query that
perfectly classifies the given examples but rather a query with good
generalisation properties on unseen objects.

Other supervised learning tasks that cannot be formulated as easily in
this framework can still be characterized by the type of target
attribute. Cases where there are several nominal attributes (possibly
with hierarchical dependencies) correspond to (hierarchical)
multi-label classification tasks, while settings with an ordinal
target corresponds the problem of learning a ranking and with a
real-valued target to a regression problem.

One technique to solve binary and multi-class classification problems
is to use a descision tree.  Each branch in the decision tree
correspond to a conjunction of literals and all branches ending on
leaves corresponding to the same target value can be combined together
using disjunctions. Thus, a decision tree can be seen as a set of
classifing queries in DNF. Decision trees can be used with data
including binary, nominal and real-valued attribute by constructing
the literals to locally optimize an information-based criterion.

In the presence of Boolean attributes and target, Logical Analysis of
Data (LAD) \cite{boros00implementation} aims at finding a perfect
classifier of fixed form, e.g. a horn clause, a DNF, CNF, linear or
quadratic Boolean formula.

The common aim of mining Emerging Patterns, Constrast Set Mining and Subgroup Discovery
is to find queries whose support is distributed very unevenly with respect to the target attribute.

In the case of Emerging Patterns, considering Boolean data and
monotone conjunctive queries, i.e. itemset mining, this means finding
itemsets whose presence is statistically dependent on the positive or negative labelling of
the objects. In the extreme case, the itemset would be present only in the positive example and would form a perfect classifier for the data at hand. However, this is not generally the case. 

Similarly, given a nominal target attribute, the goal of Contrast Set
Mining is to find a monotone conjunctive query that best discriminate
between the objects from one class and the other objects.

Subgroup discovery, in a more general sense, aims at finding 

Exceptional Model mining (model on the target attributes, not just a single one)


\begin{tabular}{c@{\hspace*{0.5em}}c@{\hspace*{0.5em}}c@{\hspace*{0.5em}}p{3cm}@{\hspace*{0.5em}}p{3cm}}
Method & Attributes & Target & Language & Aim \\
FCA & Boolean & - & Closed monotone conjunctions & Study the Galois connection \\
FIM & Boolean & - & Monotone conjunctions & Find all queries with support larger than a chosen threshold \\
LAD & Boolean & Binary & Boolean formulae of fixed type (Horn clause, DNF, CNF,...) & Find a perfect classifier \\
DT & Boolean & Binary & Boolean formulae in DNF & Find a classifier \\
EM & Boolean & Binary & Monotone conjunctions & subgroups which are distributed very un-envenly among the two sets \\ 
CSM & Nominal & Nominal & Monotone conjunctions & Contrast one group against the others (classifies..) \\
SD & Nominal & Numerical & Monotone conjunctions & Describe subgroups for which the target attribute values differ from the rest of the objects \\
EMM & Nominal & Arbitrary & Monotone conjunctions & subgroup and model over the target such that the model differs significantly for the subgroup and the rest of the objects
\end{tabular}

 



Given two subsets of entities, typically labeled as positive and
negative examples, described by the same set of variables, identifying
discriminative patterns is the main focus of several different
techniques: Logical Analysis of Data (LAD)
\cite{boros00implementation}, Emerging Patterns Mining (EPM), Contrast
Set Mining (CSM) or Subgroup Discovery (SD), to name a few (see
\cite{kralj09supervised} for a unifying survey of the last three).

Attempts have been made to extend Subgroup Discovery from single
binary class label to multiple output variables.  In
\cite{umek09subgroup}, this is done by first applying k-medoids
clustering to input and output variables separately and then finding
relationships between the clusters using chi-square test of
independence.  In the framework defined by Leman et
al. \cite{leman08exceptional} and its recent
instance\cite{vanleeuwen10maximal}, the aim is more generally to
identify a subset of the entities defined by a single binary feature
or a pattern over several features such that a model fitted to that
subset significantly differs from the same model fitted to the rest of
the entities.

The approach presented in \cite{garriga07crossmining} somewhat
similarly uses frequent itemsets on the binary attributes to define
subsets of the entities and then try to form a partition of the
original data with the subsets that can be best modeled using the
numerical attributes.

Redescription mining differs from these techniques in that it aims at
simultaneously finding multiple descriptions of a subset of entities
which is not previously specified, selecting the few relevant among a
potentially large set of variables.  This problem was introduced
in~\cite{ramakrishnan04turning}, and has since attained continuous
research interest
(e.g.~\cite{parida05redescription,zaki05reasoning,gallo08finding,kumar07redescription}).
The algorithms proposed for redescription mining have been based on
various ideas, including decision trees~\cite{ramakrishnan04turning},
co-clusters~\cite{parida05redescription}, and frequent
itemsets~\cite{gallo08finding}.


\section{Extension to Real-Valued Data}
Until now, the redescription mining algorithms
(see~\cite{gallo08finding,parida05redescription,ramakrishnan04turning,zaki05reasoning})
have not been able to handle other than Boolean data.  This restricts
the range of possible applications or makes discretization a
prerequisite, entailing a possibly harmful loss of information. In
niche-finding, for example, while the fauna can be naturally
represented using a Boolean presence/absence data, the weather cannot.

In \cite{galbrun11from}, we extended redescription mining to
real-valued data using a surprisingly simple and efficient
approach. We provided extensive experimental evaluation to study the
behaviour of the proposed algorithm and showed the statistical
significance of our results using recent innovations on randomization
methods.

\section{Redescription Mining in the Frequent Pattern Mining Toolbox}
As we already mentionned, redescription mining has close ties with
various other data mining techniques, for example, Subgroup Discovery,
Emerging Patterns, Exceptional Models, correlation analysis,
multi-label classifiers, etc. It would be interesting to clarify these
connections.

The accuracy of a redescription is generaly measured by the Jaccard
coefficient. The statistical significance of the redescription is also
a factor affecting its interestingness. There are many properties
which affects differenct aspects of the quality of redescription:
accuracy, interestingness, interpretability, redundancy, etc. How to
best evaluate them and combine them to select the best redescriptions
is an open question.

\section{Theoretical study of Redescription Mining}
When allowing for the use of disjunctions, the search space is no
longer monotone and it becomes impossible to exploit this property to
design efficient mining algorithms.  Yet, how accurate a second level
redescription $(v_1 \lor v_2, w_1)$ can be given upper bounds on the
accuracies of $(v_1, w_1)$ and $(v_2, w_1)$?  Obtaining bounds of this
sort could help find second level rules, i.e. redescription $(v_1 \lor
v_2, w_1)$ is accurate but neither $(v_1, w_1)$ nor $(v_2, w_1)$ are
sufficiently.

Solving the redescription mining with arbitrary Boolean formulae is
clearly a difficult problem, but a more precise analysis of its
complexity would be helpful.  A possible direction for research would
be to design an exponential algorithm for solving this problem
exactly, possibly using time and space trade-offs could shed .

\section{Applications}
There is of course space for improvement in the application of our
method to niche-finding, in collaboration with biologists.  One
example is the extension to trait data, that is, to finding
redescriptions where the query over species is replaced by a more
general query over the physiological characteristics of species.

An other domain where the application of redescription mining could be
investigated is that of gene set enrichment and micro-array data
analysis. For instance, could \emph{templates redescriptions}, where
the form of the query on one side would be fixed, to, say, $v_1 < k_1
\land v_2>k_2 \land v_3>k_3$, and we would try to find as accurate
redescription while maximizing $k_2$, $k_3$ and minimizing $k_1$ help
find descriptions of genes for specific over/under-expression
patterns?

\section{Generalized redescriptions}
Finally, one could explore the generalisation of redescriptions to
other settings. Examples include multiple datasets with different
mappings of the rows and columns (\emph{re$^x$-descriptions}),
tensors, graphs.  How to define such redescriptions and how to find
them?

\bibliographystyle{siam}
\bibliography{redesc}


\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
